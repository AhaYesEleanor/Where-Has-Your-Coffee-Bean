{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "#from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.gaussian_process.kernels import ExpSineSquared\n",
    "\n",
    "from hyperopt import fmin, hp, tpe, Trials\n",
    "\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipe_from_params(params):\n",
    "    '''\n",
    "    Function used to simplify hyperopt score_fn implementation.\n",
    "    Takes in a dictionary of model parameters (including model choice).\n",
    "    Returns an unfitted pipeline.\n",
    "    '''\n",
    "    model_params = deepcopy(params)\n",
    "    \n",
    "    for key in params.keys():\n",
    "        if type(model_params[key]) == dict:\n",
    "            sub_dict = model_params.pop(key)\n",
    "            model_params.update(sub_dict)\n",
    "    \n",
    "    model_type = model_params.pop('type')\n",
    "    \n",
    "    if model_type == 'rfc':\n",
    "        model_params['max_depth'] = int(model_params['max_depth'])\n",
    "        model_params['n_estimators'] = int(model_params['n_estimators'])\n",
    "        model = RandomForestClassifier(n_jobs=3,\n",
    "                                       **model_params)\n",
    "    elif model_type == 'svc':\n",
    "        model = SVC(probability=True,\n",
    "                   **model_params)\n",
    "        \n",
    "    elif model_type == 'sgd':\n",
    "        model = SGDClassifier(loss='log',\n",
    "                              n_jobs=3,\n",
    "                              **model_params)\n",
    "        \n",
    "    elif model_type == 'gpc':\n",
    "        model = GaussianProcessClassifier(RBF(model_params['length_scale']))\n",
    "        \n",
    "    elif model_type == 'dtc':\n",
    "        model_params['max_depth'] = int(model_params['max_depth'])\n",
    "        model = DecisionTreeClassifier(**model_params)\n",
    "    \n",
    "    pipe = make_pipeline(StandardScaler(), model)\n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define categorical options for hyperparameters\n",
    "class_weight_options = (None,\"balanced\")\n",
    "svc_kernel_options = ('linear','rbf', 'poly', 'sigmoid')\n",
    "svc_kernel_dicts = [{'kernel': 'linear'}, ]\n",
    "sgd_penalty_options = ('l1', 'l2', 'elasticnet')\n",
    "criterion_options = ('gini', 'entropy')\n",
    "model_options = ('rfc', 'svc', 'sgd', 'gpc', 'dtc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_space = hp.choice('model_type',\n",
    "                      [{'type': 'rfc',\n",
    "                        'max_depth': hp.quniform('rfc_max_depth', 2, 10, 1),\n",
    "                        'class_weight': hp.choice('rfc_class_wt', class_weight_options),\n",
    "                        'n_estimators': hp.quniform('rfc_n_ests', 5, 75, 5),\n",
    "                        'criterion': hp.choice('rfc_criterion', criterion_options),\n",
    "                        'min_samples_leaf': hp.uniform('rfc_min_samples', 0.05, 0.5),\n",
    "                       },\n",
    "                       {'type': 'svc',\n",
    "                        'C': hp.lognormal('svc_C', 0, 1),\n",
    "                        'class_weight': hp.choice('svc_class_wt', class_weight_options),\n",
    "                        'kernel': hp.choice('svc_kernel', \n",
    "                                            [{'kernel': 'linear'},\n",
    "                                             {'kernel': 'rbf',\n",
    "                                              'gamma': hp.uniform('svc_gamma_rbf', 0.01, 1)},\n",
    "                                             {'kernel': 'poly',\n",
    "                                              'degree': hp.quniform('svc_degree', 2, 5, 1),\n",
    "                                              'gamma': hp.uniform('svc_gamma_poly', 0.01, 1)},\n",
    "                                             {'kernel': 'sigmoid',\n",
    "                                              'gamma': hp.uniform('svc_gamma_sigmoid', 0.01, 1)}\n",
    "                                            ])   \n",
    "                       },\n",
    "                       {'type': 'sgd',\n",
    "                        'alpha': hp.uniform('sgd_alpha', 0.01, 1),\n",
    "                        'penalty': hp.choice('sgd_penalty', sgd_penalty_options),\n",
    "                        'l1_ratio': hp.uniform('sgd_ratio', 0, 1)\n",
    "                       },\n",
    "                       {'type': 'gpc',\n",
    "                        'length_scale': hp.uniform('gpc_lscale', 0.05, 7)\n",
    "                       },\n",
    "                       #{'type': 'gnb'},\n",
    "                       {'type': 'dtc',\n",
    "                        'criterion': hp.choice('dtc_criterion', criterion_options),\n",
    "                        'max_depth': hp.quniform('dtc_max_depth', 3, 20, 1),\n",
    "                        'min_samples_leaf': hp.uniform('dtc_min_samples', 0.05, 0.5),\n",
    "                        'class_weight': hp.choice('dtc_class_wt', class_weight_options),\n",
    "                       }\n",
    "                      ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_best_model(best_model, X_test, y_test, class_dict):\n",
    "    '''\n",
    "    Takes in fitted model, test data, and dict of classes.\n",
    "    Returns accuracy and log loss for the model on the test set.\n",
    "    '''\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    y_pred_t = pd.get_dummies(pd.Series(y_pred),)\n",
    "    y_test_t = pd.Series(y_test).apply(lambda x: class_dict[x]).values\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    y_pred_prob = best_model.predict_proba(X_test)\n",
    "    y_test_n = [class_dict[country] for country in y_test]\n",
    "    y_test_n = pd.get_dummies(y_test_n)\n",
    "\n",
    "    loss = log_loss(y_test_n, y_pred_prob)\n",
    "\n",
    "    print('Test set accuracy: {} \\n Test set log loss: {}'.format(accuracy, loss))\n",
    "    return (accuracy, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hyperopt_trials(model_choice_trials, hyperopt_plot_file):\n",
    "    '''\n",
    "    Plots hyperopt trials with log loss as y axis, trial number as x axis\n",
    "    Takes in trials object populated by running hyperopt's fmin\n",
    "    Colors have been selected for colorblind readability as well as beauty\n",
    "    '''\n",
    "    losses = model_choice_trials.losses()\n",
    "    trials, model_types = zip(*((trial_n, trial_dict['misc']['vals']['model_type']) for trial_n, trial_dict in enumerate(model_choice_trials.trials)))\n",
    "    \n",
    "    x = list(trials)\n",
    "    y = list(losses)\n",
    "    \n",
    "    classifiers = ('Random Forest', 'Support Vector Machine', 'Logistic Regression', \n",
    "                   'Gaussian Process', 'Decision Tree')\n",
    "    label = list((num_list[0] for num_list in model_types))\n",
    "    colors = ['#d98f5e', '#664C3e', '#9cd81e', '#6eb6be', '#9b98fd']\n",
    "    cmap = matplotlib.colors.ListedColormap(colors)\n",
    "    \n",
    "    fig = plt.figure(figsize=(12,6))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.set_facecolor('#fffbf0')\n",
    "\n",
    "    plt.scatter(x, y, c=label, cmap=cmap, alpha=0.8)\n",
    "    plt.yscale('log')\n",
    "    plt.ylim(1.05,1.5)\n",
    "    plt.yticks([1.05, 1.1, 1.15, 1.2, 1.25, 1.3, 1.35, 1.3862 , 1.4, 1.45],\n",
    "               ['1.05', '1.1', '1.15', '1.2', '1.25', '1.3', '1.35', 'Random', '1.4', '1.45'])\n",
    "    plt.ylabel('Log Loss')\n",
    "    plt.xlabel('Trials')\n",
    "    \n",
    "    c = [mpatches.Circle((0.5, 0.5), radius = 0.25, facecolor=colors[i], edgecolor=\"none\") for i in range(len(classifiers))]\n",
    "    plt.legend(c,classifiers,loc=(1.01,0.8), facecolor='none', edgecolor='none')\n",
    "\n",
    "    plt.savefig(hyperopt_plot_file, transparent=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_pipeline(hyperopt_output):\n",
    "    '''\n",
    "    Takes in 'best_model_choice' model/hyperparameter information from hyperopt fmin.\n",
    "    Returns pipeline constructed per those optimized specifications.\n",
    "    '''\n",
    "    model_params = deepcopy(hyperopt_output)\n",
    "    \n",
    "    for key in hyperopt_output.keys():\n",
    "        if type(model_params[key]) == dict:\n",
    "            sub_dict = model_params.pop(key)\n",
    "            model_params.update(sub_dict)\n",
    "            \n",
    "    print(model_params)\n",
    "    \n",
    "    model_type = model_options[model_params.pop('model_type')]\n",
    "    \n",
    "    if model_type == 'rfc':\n",
    "        max_depth = int(model_params['rfc_max_depth'])\n",
    "        n_estimators = int(model_params['rfc_n_ests'])\n",
    "        class_weight = class_weight_options[int(model_params['rfc_class_wt'])]\n",
    "        criterion = criterion_options[int(model_params['rfc_criterion'])]\n",
    "        model = RandomForestClassifier(n_jobs=3,\n",
    "                                       max_depth = max_depth,\n",
    "                                       n_estimators = n_estimators,\n",
    "                                       class_weight = class_weight,\n",
    "                                       criterion = criterion\n",
    "                                      )\n",
    "    elif model_type == 'svc':\n",
    "        C = model_params['svc_C']\n",
    "        class_weight = class_weight_options[int(model_params['svc_class_wt'])]\n",
    "        kernel_list = ['linear', 'rbf', 'poly', 'sigmoid']\n",
    "        kernel = kernel_list[int(model_params['svc_kernel'])]\n",
    "        gamma_name = 'svc_gamma_{}'.format(kernel)\n",
    "        gamma = model_params[gamma_name]\n",
    "        if kernel == 'poly':\n",
    "            degree = model_params['svc_degree']\n",
    "            model = SVC(probability=True,\n",
    "                        C=C,\n",
    "                        class_weight=class_weight,\n",
    "                        kernel=kernel,\n",
    "                        gamma=gamma,\n",
    "                        degree=degree\n",
    "                       )\n",
    "        else:\n",
    "            model = SVC(probability=True,\n",
    "                        C=C,\n",
    "                        class_weight=class_weight,\n",
    "                        kernel=kernel,\n",
    "                        gamma=gamma\n",
    "                       )\n",
    "        \n",
    "    elif model_type == 'sgd':\n",
    "        alpha = model_params['sgd_alpha']\n",
    "        penalty = sgd_penalty_options[model_params['sgd_alpha']]\n",
    "        l1_ratio = model_params['sgd_ratio']\n",
    "        model = SGDClassifier(loss='log',\n",
    "                              n_jobs=3,\n",
    "                              alpha=alpha,\n",
    "                              l1_ratio=l1_ratio\n",
    "                             )\n",
    "        \n",
    "    elif model_type == 'gpc':\n",
    "        length_scale = model_params['gpc_lscale']\n",
    "        model = GaussianProcessClassifier(RBF(length_scale=length_scale))\n",
    "        \n",
    "    else:\n",
    "        max_depth = int(model_params['dtc_max_depth'])\n",
    "        criterion = criterion_options[model_params['dtc_criterion']]\n",
    "        min_samples_leaf = model_params['dtc_min_samples']\n",
    "        class_weight = class_weight_options[model_params['dtc_class_wt']]\n",
    "        model = DecisionTreeClassifier(**model_params)\n",
    "    \n",
    "    pipe = make_pipeline(StandardScaler(), model)\n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perm_import(fitted_pipeline, X_test, y_test, class_dict):\n",
    "    '''\n",
    "    Takes in fitted model, test set, and dictionary of class labels and binary labels\n",
    "    Returns log loss permutation importance for all features\n",
    "    '''\n",
    "    y_pred = fitted_pipeline.predict_proba(X_test)\n",
    "    y_test_n = [class_dict[country] for country in y_test]\n",
    "    y_test_n = pd.get_dummies(y_test_n)\n",
    "    \n",
    "    loss = log_loss(y_test_n, y_pred)\n",
    "    #print(loss)\n",
    "    \n",
    "    feature_importances = {}\n",
    "    for feat in X_test.columns:\n",
    "        X_test_copy = deepcopy(X_test)\n",
    "        feat_to_randomize = deepcopy(X_test[feat])\n",
    "        feat_to_randomize = feat_to_randomize.sample(frac=1)\n",
    "        feat_to_randomize.reset_index(inplace=True, drop=True)\n",
    "        X_test_copy.loc[:,feat] = feat_to_randomize.values\n",
    "        \n",
    "        y_pred = fitted_pipeline.predict_proba(X_test_copy)\n",
    "        feat_loss = log_loss(y_test_n, y_pred)\n",
    "        feat_import = feat_loss - loss\n",
    "        feature_importances[feat] = feat_import\n",
    "    return feature_importances\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confmx(y_test, y_pred, labels, confmx_filename):\n",
    "    '''\n",
    "    plots confusion matrix and saves it to file\n",
    "    '''\n",
    "    conf_mat = confusion_matrix(y_test, y_pred, labels)\n",
    "    plt.figure(figsize = (8,5.5))\n",
    "    conf_heatmap = sns.heatmap(conf_mat, annot=True, xticklabels=labels, \n",
    "                               yticklabels=labels,\n",
    "                               cmap = sns.color_palette(\"GnBu_d\"))\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    conf_heatmap.figure.savefig(confmx_filename, transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_jitter(arr):\n",
    "    stdev = .005*(max(arr)-min(arr))\n",
    "    return arr + np.random.randn(len(arr)) * stdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data_by_feats(x_feat, y_feat, X, y, class_dict, feat_plot_filename):\n",
    "    '''\n",
    "    Takes in two features of interest, full dataset X, and a dictionary relating class labels to class id numbers\n",
    "    Plots X data along specified feature dimensions\n",
    "    '''\n",
    "    x = X[x_feat]\n",
    "    x = rand_jitter(x)\n",
    "    y = X[y_feat]\n",
    "    y = rand_jitter(y)\n",
    "    label = pd.Series(X['Country.of.Origin']).apply(lambda x: class_dict[x])\n",
    "    colors = ['#d98f5e', '#664C3e', '#6eb6be', '#9b98fd']\n",
    "    countries = ['Brazil','Colombia','Guatemala','Mexico']\n",
    "\n",
    "    fig = plt.figure(figsize=(6,4.5))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.set_facecolor('#fffbf0')\n",
    "\n",
    "    plt.scatter(x, y, c=label, cmap=matplotlib.colors.ListedColormap(colors), alpha=0.5)\n",
    "    #plt.xlim(6.5,8.5)\n",
    "    plt.xlabel('Rating: '+ x_feat)\n",
    "    plt.ylabel('Rating: '+ y_feat)\n",
    "\n",
    "    c = [mpatches.Circle((0.5, 0.5), radius = 0.25, facecolor=colors[i], edgecolor=\"none\") for i in range(len(countries))]\n",
    "    plt.legend(c,countries,loc=(1.01,0.5), facecolor='none', edgecolor='none')\n",
    "    plt.savefig(feat_plot_filename, transparent=True)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_performance_for_feature(feature, X_test, y_test, y_pred, class_dict, perf_by_feat_filename):\n",
    "    '''\n",
    "    Takes in one feature of interest, test data, predicted values, and dictionary relating class labels to class id numbers\n",
    "    Plots test data, organized by true label, color-coded by predicted label, with selected feature values as x axis\n",
    "    '''\n",
    "    x = X_test[feature]\n",
    "    x = rand_jitter(x)\n",
    "    y = y_test.apply(lambda x: class_dict[x])\n",
    "    y = rand_jitter(y)\n",
    "    label = pd.Series(y_pred).apply(lambda x: class_dict[x])\n",
    "    colors = ['#d98f5e', '#664C3e', '#6eb6be', '#9b98fd']\n",
    "    countries = ['Brazil','Colombia','Guatemala','Mexico']\n",
    "\n",
    "    fig = plt.figure(figsize=(8,4))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.set_facecolor('#fffbf0')\n",
    "\n",
    "    plt.scatter(x, y, c=label, cmap=matplotlib.colors.ListedColormap(colors), alpha=0.7)\n",
    "\n",
    "    plt.yticks([0,1,2,3,4], countries)\n",
    "    plt.ylabel('True Country of Origin')\n",
    "    plt.xlabel('Rating: '+feature)\n",
    "\n",
    "    cb = plt.colorbar()\n",
    "    cb.set_label('Predicted', rotation=270)\n",
    "    loc = np.arange(0,max(label),max(label)/float(len(colors)))\n",
    "    cb.set_ticks([0.5,1,2,2.5])\n",
    "    cb.set_ticklabels(countries)\n",
    "    plt.savefig('label_scatter1.png', transparent=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
